## Clean individual genetic data - Ver 2
# Updated version of gsiIndProbsClean.R that uses new flat file with full
# probabilities (i.e. not trimmed to 5 stocks max) for WCVI commercial troll 
# fishery and trimmed probabilities for JDF, SoG, and JS rec fisheries
# May 26, 2020

library(tidyverse)


## COMMERCIAL GSI --------------------------------------------------------------

dat_raw_comm <- read.csv(here::here("data", "gsiCatchData", "commTroll", 
                              "wcviIndProbsLong_RAW.txt"), 
                   stringsAsFactors = FALSE)

# Big chunk of code to separate ID variable into meaningful individual vectors
id_vec <- dat_raw_comm$szLineInfo %>% 
  as.vector() %>% 
  strsplit(., split = " ") %>% 
  unlist() %>% 
  matrix(., nrow = 5, ncol = length(dat_raw_comm$szLineInfo)) %>%
  t() %>% 
  data.frame() %>% 
  rename("statArea" = X1, "year" = X2, "gear" = X3, "jDay" = X4,
         "fishNum" = X5) %>% 
  mutate(
    jDay = as.numeric(as.character(jDay)),
    statArea = 
      case_when(
         statArea %in% c("Area023", "Area23", "Area_23") ~ "23",
         statArea %in% c("Area123", "Area123SWVI", "Area123Comm") ~ "123",
         statArea %in% c("Area124", "Area124SWVI", "Area124Comm", "Area123-124",
                         "Area124_24") ~ "124",
         statArea %in% c("Area125", "Area125NWVI") ~ "125",
         statArea %in% c("Area126", "Area126NWVI", "Area125-126", 
                         "Area126-127") ~ "126",
         statArea %in% c("Area127", "Area127NWVI") ~ "127",
         statArea %in% c("Area026") ~ "26",
         statArea %in% c("Area24", "Area_24", "Area_24xgill") ~ "24",
         TRUE ~ as.character(statArea)
         ),
    abbYear = sapply(strsplit(as.character(year), '[()]'), 
                function(x) (x)[2]),
    year = paste("20", abbYear, sep = ""),
    #adjust sampling day to correct for errors by genetics lab and 
    jDay = 
      case_when(
        statArea == "126" & year == "2012" & jDay > 48 &
          jDay < 116 ~ 48,
        statArea == "126" & year == "2012" & jDay > 116 &
          jDay < 121 ~ 116,
        TRUE ~ jDay),
    date = as.Date(as.numeric(as.character(jDay)),
                   origin = as.Date(paste(year, "01", "01", sep = "-"))),
    month = lubridate::month(as.POSIXlt(date, format="%Y-%m-%d")),
    week = lubridate::week(as.POSIXlt(date, format="%Y-%m-%d")),
    weekDay = weekdays(date),
    fishNum = as.numeric(as.character(fishNum))
    ) %>% 
  # adjust sampling week
  # for specific strata based on when samples were landed relative to julian 
  # date of when fishing occurred (based on reviewing FOS database and pers. 
  # comm. Lee Kearey - South Coast)
  mutate(
    adjWeek = case_when(
      statArea == "23" & month == "3" & year == "2013" ~ week - 1,
      statArea == "24" & month == "5" & year == "2013" ~ week - 1,
      statArea == "26" & month == "2" & year == "2" ~ week - 1,
      statArea == "123" & month == "6" & year == "2007" ~ week - 1,
      statArea == "123" & month == "6" & year == "2008" ~ week - 1,
      statArea == "125" & month == "6" & year == "2007" ~ week - 1,
      statArea == "125" & month == "10" & year == "2007" ~ week - 1,
      statArea == "125" & month == "5" & year == "2014"~ week - 1,
      statArea == "126" & month == "3" & year == "2007"~ week - 1,
      statArea == "126" & month == "2" & year == "2012"~ week - 1,
      statArea == "126" & month == "3" & year == "2013"~ week - 1,
      statArea == "126" & month == "4" & year == "2013"~ week - 1,
      statArea == "126" & month == "3" & year == "2015"~ week - 1,
      statArea == "127" & month == "6" & year == "2007"~ week - 1,
      statArea == "127" & month == "4" & year == "2014"~ week - 1,
      statArea == "23" & month == "6" & year == "2007"~ week - 1,
      # shift back to account for catch being harvested 3-4 days before landing
      weekDay %in% c("Sunday", "Monday") ~ week - 1,
      TRUE ~ week
      )
    ) %>% 
  #shuffle adjusted
  rename(week = adjWeek, unadjWeek = week)

#Merge id vector with original data frame and trim
dat_comm <- cbind(id_vec, dat_raw_comm) %>% 
  #calculate total summed probability for each sample
  group_by(szLineInfo) %>%
  mutate(stock = toupper(szStock),
         totalProb = sum(dProb)) %>% 
  ungroup() %>% 
  mutate(adj_prob = dProb / totalProb) %>% 
  select(-iRun, -iSample, -iYearMix, id = szLineInfo, stock,  -szStock,
         prob = dProb, adj_prob, -totalProb, -szExclude, -iRegionId, -unadjWeek,
         Region1Name = szRegion, -weekDay) %>% 
  rename(area = statArea, fish_num = fishNum) 


## Export list of stocks to be passed to makeFullStockKey script in
# stockKey repo 
# stks_out <- dat_comm %>%
#   select(stock, Region1Name) %>%
#   distinct()
# saveRDS(stks_out, here::here("data", "stockKeys", "wcviTrollStocks.rds"))

# stock key generated in stockKey repo
stockKey <- readRDS(here::here("data", "stockKeys", "finalStockList_May2020.rds"))

comm_long <- dat_comm %>%
  select(-Region1Name) %>%
  left_join(., stockKey, by = c("stock")) %>% 
  arrange(area, year, month, jDay, fish_num) %>% 
  mutate(
    season_c = case_when(
      month %in% c("12", "1", "2") ~ "w",
      month %in% c("3", "4", "5") ~ "sp",
      month %in% c("6", "7", "8") ~ "su",
      month %in% c("9", "10", "11") ~ "f"
    ),
    season = fct_relevel(season_c, "sp", "su", "f", "w"),
    month_n = as.numeric(month),
    month = as.factor(month_n),
    year =  as.factor(year),
    pres = 1,
    area_n = as.numeric(as.character(area)),
    region = case_when(
      area_n < 125 & area_n > 27 ~ "SWVI",
      area_n < 25 ~ "SWVI",
      TRUE ~ "NWVI"
    ),
    region = as.factor(region), 
    area = as.factor(area),
    temp_strata = paste(month_n, region, sep = "_")
    ) %>% 
  select(id, fish_num, temp_strata, region, area, year, month, week, jDay, date,
         gear, pres, season, month_n, area_n, adj_prob, stock, 
         Region1Name:pst_agg) %>% 
  arrange(year, region, id, desc(adj_prob))

saveRDS(comm_long, here::here("data", "gsiCatchData", "commTroll",
                         "wcviIndProbsLong.rds"))
# comm_long <- readRDS(here::here("data", "gsiCatchData", "commTroll",
#                            "wcviIndProbsLong.rds"))


## Clean recreation GSI data ---------------------------------------------------

rec_full <- read.csv(here::here("data", "gsiCatchData", "rec", 
                                "rec_gsi_may2020.txt"), stringsAsFactors = F)

# pull stocks to add to stockkey repo
# stk_out <- rec_full %>% 
#   filter(!DNA_RESULTS_STOCK_1 == "",
#          !is.na(PROB_1)) %>% 
#   select(s1 = DNA_RESULTS_STOCK_1, s2 = DNA_STOCK_2, s3 = DNA_STOCK_3, 
#          s4 = DNA_STOCK_4, s5 = DNA_STOCK_5, REGION_1_ROLLUP) %>% 
#   pivot_longer(., cols = s1:s5, names_to = "rank", values_to = "stock") %>%
#   select(stock, sc_reg1 = REGION_1_ROLLUP) %>% 
#   filter(!stock == "") %>% 
#   distinct()
# saveRDS(stk_out, here::here("data", "stockKeys", "rec_gsi_stocks.rds"))

stockKey <- readRDS(here::here("data", "stockKeys", "finalStockList_June2020.rds"))

# data frame of probabilities
temp_prob <- rec_full %>% 
  filter(!DNA_RESULTS_STOCK_1 == "",
         RESOLVED_STOCK_SOURCE == "DNA") %>% 
  rename(p1 = PROB_1, p2 = PROB_2, p3 = PROB_3, p4 = PROB_4, p5 = PROB_5) %>% 
  pivot_longer(., cols = c(p1, p2, p3, p4, p5),
               names_to = "rank_prob", values_to = "prob") %>%  
  select(BIOKEY, COLLECTION_DATE, rank_prob, prob)

# data frame of stock IDs
rec_long <- rec_full %>% 
  filter(!DNA_RESULTS_STOCK_1 == "",
         RESOLVED_STOCK_SOURCE == "DNA") %>% 
  rename(s1 = DNA_RESULTS_STOCK_1, s2 = DNA_STOCK_2, s3 = DNA_STOCK_3, 
         s4 = DNA_STOCK_4, s5 = DNA_STOCK_5,
         p1 = PROB_1, p2 = PROB_2, p3 = PROB_3, p4 = PROB_4, p5 = PROB_5) %>% 
  pivot_longer(., cols = c(s1, s2, s3, s4, s5), 
               names_to = "rank", values_to = "stock") %>% 
  cbind(., temp_prob %>% select(rank_prob, prob)) %>% 
  #add regional roll ups
  left_join(., stockKey, by = "stock") %>% 
  filter(!is.na(prob)) %>%
  mutate(date = as.Date(as.numeric(as.character(DAYOFYEAR - 1)),
                        origin = as.Date(paste(YEAR, "01", "01", sep = "-"))),
         month = lubridate::month(as.POSIXlt(date, format="%Y-%m-%d")),
         week = lubridate::week(as.POSIXlt(date, format="%Y-%m-%d"))
  ) %>% 
  #adjust probabilities 
  group_by(BIOKEY) %>% 
  mutate(total_prob = sum(prob),
         adj_prob = prob / total_prob) %>% 
  ungroup() %>% 
  mutate(
    region = case_when(
      PFMA > 124 ~ "NWVI",
      PFMA < 28 & PFMA > 24 ~ "NWVI",
      PFMA %in% c("20", "121", "21") ~ "Juan de Fuca Strait",
      is.na(PFMA) ~ "Juan de Fuca Strait",
      PFMA < 125 & PFMA > 120 ~ "SWVI",
      PFMA < 25 & PFMA > 20 ~ "SWVI",
      # PFMA < 20 & PFMA > 12 ~ "Georgia Strait",
      # PFMA %in% c("28", "29") ~ "Georgia Strait",
      PFMA %in% c("10", "11", "12", "111") ~ "Johnstone Strait",
      PFMA %in% c("14", "15", "16") ~ "N. Strait of Georgia",
      PFMA %in% c("17", "18", "19", "28", "29") ~ "S. Strait of Georgia",
      PFMA %in% c("10", "11", "111") ~ "Queen Charlotte Sound",
      PFMA %in% c("12", "13") ~ "Queen Charlotte and\nJohnstone Straits"
    ),
    season_c = case_when(
      month %in% c("12", "1", "2") ~ "w",
      month %in% c("3", "4", "5") ~ "sp",
      month %in% c("6", "7", "8") ~ "su",
      month %in% c("9", "10", "11") ~ "f"
    ),
    legal_lim = case_when(
      PFMA < 20 & PFMA > 11 ~ 620,
      PFMA %in% c("28, 29") ~ 620,
      TRUE ~ 450
    ),
    legal = case_when(
      LENGTH_MM >= legal_lim ~ "legal",
      LENGTH_MM < legal_lim ~ "sublegal",
      KEPTREL == "Kept" ~ "legal",
      KEPTREL == "Rel" ~ "sublegal"
    ),
    season = fct_relevel(season_c, "sp", "su", "f", "w"),
    month_n = as.numeric(month),
    month = as.factor(month_n),
    year =  as.factor(YEAR),
    pres = 1,
    area_n = as.numeric(as.character(PFMA)),
    region = as.factor(region), 
    area = case_when(
      is.na(PFMA) ~ "20_121_21",
      TRUE ~ as.character(PFMA) 
    ),
    area =  as.factor(area),
    temp_strata = paste(month_n, region, sep = "_"),
    gear = "sport"
  ) %>% 
  select(id = BIOKEY, fish_num = FISH_NO, temp_strata, region, area, 
         subarea = SUBAREA, 
         year, month, week, jDay = DAYOFYEAR, date, gear = gear, 
         fl = LENGTH_MM, release = KEPTREL, legal, sex = SEX, pres, season, 
         month_n, area_n, adj_prob, stock, Region1Name:pst_agg) %>% 
  arrange(year, region, id, desc(adj_prob))

saveRDS(rec_long, here::here("data", "gsiCatchData", "rec",
                            "recIndProbsLong.rds"))

tt2 <- rec_long %>%
  # filter(!region %in% c("Juan de Fuca Strait", "S. Strait of Georgia")) %>%
  filter(area %in% c("11", "111", "12", "13")) %>%
  select(region, month, area, id) %>%
  distinct() %>%
  droplevels()
table(tt2$area, tt2$month)

ggplot(rec_long) + 
  geom_histogram(aes(fl)) +
  facet_wrap(~legal)

rec_long %>% 
  filter(is.na(legal)) %>% 
  group_by(release) %>% 
  tally()


### FOLLOWING IS DEPRECATED ###


## Roll up to regional aggregates ----------------------------------------------

#cleaning function to filter out non-dominant assignments below threshold
clean_dat <- function(dat, threshold = 0.75) {
  dat %>% 
    group_by(id) %>% 
    mutate(max_assignment = max(aggProb)) %>% 
    # Remove samples where top stock ID is less than 75% probability
    filter(!aggProb < max_assignment, 
           !max_assignment < threshold) %>% 
    ungroup() %>% 
    distinct() %>% 
    mutate(
      season_c = case_when(
        month %in% c("12", "1", "2") ~ "w",
        month %in% c("3", "4", "5") ~ "sp",
        month %in% c("6", "7", "8") ~ "su",
        month %in% c("9", "10", "11") ~ "f"
      ),
      season = fct_relevel(season_c, "sp", "su", "f", "w"),
      month_n = as.numeric(month),
      month = as.factor(month_n),
      year =  as.factor(year),
      pres = 1,
      area_n = as.numeric(as.character(statArea)),
      catchReg = case_when(
        area_n < 125 & area_n > 27 ~ "SWVI",
        area_n < 25 ~ "SWVI",
        TRUE ~ "NWVI"
      ),
      catchReg = as.factor(catchReg), 
      statArea = as.factor(statArea)) 
}

# Region 3 first (i.e. large regional aggregats)
reg3_unfiltered <- dat2 %>% 
  dplyr::rename(regName = Region3Name) %>% 
  # aggregate based on likely stocks of interest
  mutate(regName = fct_recode(regName, ECVI = "SOG"),
         regName = as.character(regName),
         regName = case_when(
           regName %in% c("Columbia", "Snake") ~ "Columbia",
           regName %in% c("Coastal Washington", "Washington Coast",
                          "Alaska South SE", "North/Central BC", "SOG", 
                          "Oregon/California", "ECVI", "WCVI") ~ "Other",
           TRUE ~ regName
         ),
         regName = as.factor(abbreviate(regName, minlength = 5))
  )  %>% 
  group_by(id, regName) %>% 
  dplyr::summarize(aggProb = sum(adjProb)) %>% 
  dplyr::arrange(id, desc(aggProb)) %>% 
  left_join(dat %>% 
               select(id, statArea, year, month, week, jDay, gear, 
                      fishNum, date),
             .,
             by = "id") %>% 
  distinct() 

#remove non-dom assignments based on above
reg3 <- reg3_unfiltered %>% 
  clean_dat(., threshold = 0.75)

#how many samples retained based on threshold
length(unique(reg3$id)) / length(unique(dat2$id))

saveRDS(reg3_unfiltered, here::here("data", "gsiCatchData", "commTroll",
                         "reg3RollUpCatchProb_unfiltered.RDS"))
saveRDS(reg3, here::here("data", "gsiCatchData", "commTroll",
                           "reg3RollUpCatchProb.RDS"))

#key for adding aggregates to below 
reg3_key <- reg3 %>%
  select(id, aggName = regName)

# Region 1 next (approximately equivalent to fine-scale PSC groupings eg MUFR)
reg1 <- dat2 %>% 
  group_by(id, Region1Name) %>% 
  dplyr::summarize(aggProb = sum(adjProb)) %>% 
  dplyr::arrange(id, desc(aggProb)) %>% 
  left_join(dat %>% 
              select(id, statArea, year, month, week, jDay, gear, 
                     fishNum, date),
            .,
            by = "id") %>% 
  distinct() %>% 
  dplyr::rename(pscName = Region1Name) %>% 
  clean_dat() %>% 
  left_join(.,
            reg3_key,
            by = "id")

saveRDS(reg1, here::here("data", "gsiCatchData", "commTroll",
                         "reg1RollUpCatchProb.RDS"))

# Modified region 1 with Fraser focus
fr_reg1 <- reg1 %>%
  mutate(
    regName = case_when(
      aggName == "FrsrR" & grepl("TH", pscName) ~ "Thomp-Early",
      pscName %in% c("LWFR-Su", "LWFR-Sp", "MUFR", "UPFR") ~ "FR-Early",
      pscName == "LWFR-F" ~ "LWFR-Late",
      aggName == "FrsrR" ~ pscName,
      TRUE ~ "Other"
    )
  ) 
saveRDS(fr_reg1, here::here("data", "gsiCatchData", "commTroll",
                            "reg1RollUpCatchProb_Fraser.RDS"))
# alternative option w/ finer resolution 
fr_reg1B <- reg1 %>%
  mutate(
    regName = case_when(
      pscName == "SOTH" ~ "South Thomp.",
      pscName %in% c("LWTH", "NOTH") ~ "Thomp. 1.x",
      pscName %in% c("LWFR-Su", "LWFR-Sp", "MUFR", "UPFR") ~ "FR 1.x",
      pscName == "LWFR-F" ~ "FR-Fall",
      aggName == "FrsrR" ~ pscName,
      TRUE ~ "Other"
    )
  ) 
saveRDS(fr_reg1B, here::here("data", "gsiCatchData", "commTroll",
                            "reg1RollUpCatchProb_FraserB.RDS"))
table(fr_reg1$regName, fr_reg1$month_n)
table(fr_reg1B$regName, fr_reg1B$month_n)


# PST aggregates next (based on figures in 2018 Appendix E)
pst_aggs <- dat2 %>% 
  group_by(id, pst_agg) %>% 
  dplyr::summarize(aggProb = sum(adjProb)) %>% 
  dplyr::arrange(id, desc(aggProb)) %>% 
  left_join(dat %>% 
              select(id, statArea, year, month, week, jDay, gear, 
                     fishNum, date),
            .,
            by = "id") %>% 
  distinct() %>% 
  dplyr::rename(pstName = pst_agg) %>% 
  clean_dat() 

saveRDS(pst_aggs, here::here("data", "gsiCatchData", "commTroll",
                             "pstAggRollUpCatchProb.RDS"))


# Compare to catch data --------------------------------------------------------
#Add weekly catches and sampling effort
dailyCatch <- readRDS(here::here("data", "gsiCatchData", "commTroll",
                    "dailyCatch_WCVI.rds")) %>% 
  dplyr::rename(statArea = area) %>% 
  mutate(statArea = as.character(statArea),
         date = as.Date(as.numeric(as.character(jDay)), 
                        origin = as.Date(paste(year, "01", "01", sep = "-"))),
         month = lubridate::month(as.POSIXlt(date, format="%Y-%m-%d")),
         week = lubridate::week(as.POSIXlt(date, format="%Y-%m-%d")))
weeklyCatch <- dailyCatch %>% 
  select(-catchReg) %>%
  select(-jDay, - date, -cpue) %>% 
  dplyr::group_by(statArea, year, month, week) %>% 
  dplyr::summarize(weeklyCatch = sum(catch),
                   weeklyEffort = sum(boatDays))
write.csv(weeklyCatch, here::here("data", "gsiCatchData", "commTroll",
                                   "weeklyCatch_WCVI.csv"), row.names = FALSE)


# review sampling effort relative to catch
dailySamples <- dat2 %>% 
  group_by(statArea, year, month, jDay) %>% 
  mutate(nSampled = length(unique(fishNum))) %>% 
  ungroup() %>%
  select(statArea, year, month, week, jDay, nSampled) %>% 
  distinct() %>% 
  mutate(year = as.numeric(year))
weeklySamples <- dailySamples %>% 
  group_by(statArea, year, month, week) %>% 
  summarize(nSampled = sum(nSampled)) %>% 
  ungroup()

summDat <- weeklyCatch %>%
  full_join(.,
            # adjWeeklySamps,
            weeklySamples,
            by = c("statArea", "year", "week", "month")) %>% 
  replace_na(list(nSampled = 0, weeklyCatch = 0, weeklyEffort = 0)) %>% 
  mutate(sampPpn = nSampled / weeklyCatch) %>% 
  # filter(statArea %in% unique(reg3$statArea)) %>% #constrain to focal stat areas
  distinct()

## Sampling proportion exceeds 100% because at least some samples are from Taaq
# fishery; ideally include effort or at least catch from that sector
## Double check this...
dum2 <- summDat %>% 
  filter(sampPpn > 1) %>% 
  arrange(statArea, year) 

dailyCatch %>% 
  filter(statArea == "24", month == "6", year == "2013") %>% 
  select(statArea:jDay, week, catch, date)
dailySamples %>% 
  filter(statArea == "24", month == "6", year == "2013")

# weeklyCatch %>% 
#   filter(statArea == "125", month == "9", year == "2012") %>% 
#   select(-weeklyEffort)
# weeklySamples %>% 
#   filter(statArea == "125", month == "9", year == "2012")

write.csv(dum2, here::here("data", "gsiCatchData", "commTroll", 
                           "missingCatchData.csv"), row.names = FALSE)

ggplot(summDat, aes(x = week, y = sampPpn)) +
  geom_point() +
  facet_wrap(~statArea, scales = "free_y") +
  theme_bw()
ggplot(summDat, aes(x = week, y = nSampled)) +
  geom_point() +
  facet_wrap(~statArea, scales = "free_y") +
  theme_bw()